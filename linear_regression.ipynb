{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Hàm mất mát của Linear Regression là:\n",
    "$~~~\\mathcal{L}(\\mathbf{w,b}) = \\frac{1}{2N}||\\mathbf{y - (\\bar{X}w} +b)||_2^2$\n",
    "\n",
    "Đạo hàm của hàm mất mát là:\n",
    "$~~~\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w,b}) = \\frac{1}{N}\\mathbf{\\bar{X}}^T \\mathbf{((\\bar{X}w + b) - y)} ~~~~~(1)$\n",
    "\n",
    "$~~~\\nabla_{\\mathbf{b}}\\mathcal{L}(\\mathbf{w,b}) = \\frac{1}{N}\\mathbf((\\bar{X}w + b) - y) ~~~~~(2)$\n",
    "\n",
    "Cập nhật gradient cho một biến: $~~~~~x_{t+1} = x_{t} - \\eta f’(x_{t})$\n",
    "\n",
    "=> Cập nhật gradient cho Weight và bias:\n",
    "    $\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha\\nabla_{w}~~~$;\n",
    "    $~~~~~\\mathbf{b} \\leftarrow \\mathbf{b} - \\alpha\\nabla_{b}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "instead of simply aiming to minimize loss (empirical risk minimization):\n",
    "$$\\text{minimize(Loss(Data|Model))}$$\n",
    "\n",
    "we'll now minimize loss+complexity, which is called **structural risk minimization**:\n",
    "$$\\text{minimize(Loss(Data|Model)} + \\lambda \\text{ complexity(Model))}$$\n",
    "\n",
    "Our training optimization algorithm is now a function of two terms: the loss term, which measures how well the model fits the data, and the regularization term, which measures model complexity.\n",
    "\n",
    "[Why L1 norm for sparse models](https://stats.stackexchange.com/a/159379/388752)\n",
    "\n",
    "### $L_1\\text{ - LASSO Regularization}$\n",
    "\n",
    "$$L_1\\text{ regularization term} = R(\\mathbf{w}) = ||\\boldsymbol w||_1 = \\Sigma_i |w_i| $$\n",
    "\n",
    "Graident: $$\\frac{dL_1(w)}{dw} = sign(w) \\text{ where } sign(w) = (\\frac{w_1}{|w_1|}, \\frac{w_2}{|w_2|}, \\dots, \\frac{w_m}{|w_m|})$$\n",
    "\n",
    "Notice that for $L_1$, the gradient is either 1 or -1, except for when $w_i$=0. That means that L1-regularization will move any weight towards 0 with the same step size, regardless the weight's value\n",
    "\n",
    "<img src=\"utils/img/regularization/l1.png\">\n",
    "\n",
    "<img src=\"utils/img/regularization/l1_grad.png\" width=300 height=300>\n",
    "\n",
    "### $L_2\\text{ - Ridge Regularization}$\n",
    "\n",
    "$$L_2\\text{ regularization term} = R(\\mathbf{w}) = ||\\boldsymbol w||_2^2 = {w_1^2 + w_2^2 + ... + w_n^2}$$\n",
    "\n",
    "So in conclude, the loss regularized function:\n",
    "$$J_{\\text{reg}} = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{Xw}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2$$\n",
    "\n",
    "=> gradient: \n",
    "$$\\frac{\\partial J_{\\text{reg}} }{\\partial \\mathbf{w}} = \\frac{\\partial J}{\\partial \\mathbf{w}} + \\lambda \\mathbf{w}$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicussion\n",
    "\n",
    "[When use L1, L2?](https://machinelearningcoban.com/2017/03/04/overfitting/#comment-3310034656)\n",
    "\n",
    "> Q\n",
    "- Tại sao Regularization lạ giảm được overfitting ?\n",
    "- Khi nào thì chúng ta dùng L1 và L2 . Em cảm ơn .\n",
    "\n",
    "> A\n",
    "\n",
    "Khi có regularization thì các hệ số 'bị ép' vào khuôn khổ chứ không được tự do bay nhảy nữa.\n",
    "\n",
    "L2 giúp cho các hệ số nhỏ, không được quá lớn. L1 giúp cho hầu hết các hệ số bằng 0.\n",
    "Thường thì người ta dùng L2 vì đạo hàm đẹp. L1 không có đạo hàm tại 0 và ít được sử dụng hơn. Khi thực hành, có thể chọn L2 trước xem kq thế nào, nếu tệ thì thử sang L1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression():\n",
    "    # Constructor\n",
    "    def __init__(self, learning_rate=0.01, epochs=20, regularize=None, lamda=0.01):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.regularize = regularize\n",
    "        self.lamda = lamda\n",
    "    # Fit\n",
    "    def fit(self,X, y):\n",
    "        # init parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.random.rand(n_features)\n",
    "        self.b = 0\n",
    "        threshold = 0.001\n",
    "        \n",
    "        w_best = self.w.copy()\n",
    "        # gradient descent\n",
    "        for _ in range(self.epochs):\n",
    "            \n",
    "            y_predicted = self.predict(X)\n",
    "            dW = 1/n_samples*(X.T@(y_predicted-y))\n",
    "            dB = 1/n_samples*(y_predicted-y)\n",
    "            self.w -= self.lr*dW\n",
    "            self.b -= self.lr*np.sum(dB)\n",
    "            # regularization\n",
    "            if self.regularize == 'l1':\n",
    "                self.w -= 1/n_samples*self.lamda*np.sign(self.w)\n",
    "            elif self.regularize == 'l2':\n",
    "                self.w -= 1/n_samples*self.lamda*self.w\n",
    "\n",
    "            if np.allclose(self.w, w_best, threshold): break\n",
    "            else: w_best = self.w.copy()\n",
    "        return self\n",
    "    \n",
    "    # Predict \n",
    "    def predict(self, X):\n",
    "        return X@self.w +self.b\n",
    "    \n",
    "    # Evaluate by RMSE\n",
    "    def rmse(self,y, y_predicted):\n",
    "        return 1/(2*len(y))*np.sqrt(np.sum((y-y_predicted)**2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "data = diabetes.data\n",
    "target = diabetes.target\n",
    "features=diabetes.feature_names\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# just for understand data\n",
    "df = pd.DataFrame(X_train, columns=features)\n",
    "df['target'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.026617043248765"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(regularize='l1', lamda=0.01)\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "model.rmse(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.060569566721311"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "model_lib = linear_model.LinearRegression().fit(X_train,y_train)\n",
    "\n",
    "y_predicted_lib = model_lib.predict(X_test)\n",
    "\n",
    "model.rmse(y_test, y_predicted_lib)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (train_index, test_index) in enumerate(kf):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(f\"  Train: index={train_index}\")\n",
    "#     print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "models = []\n",
    "models_rmse = []\n",
    "n_samples,n_features = data.shape\n",
    "for i in range(n_folds):\n",
    "    test_index = np.array([index for index in range(i*(n_samples // n_folds),(i+1)*(n_samples // n_folds) if (i+1)*(n_samples // n_folds) < data.shape[0] else data.shape[0])])\n",
    "    train_index = np.array(list(set([j for j in range(n_samples)]) - set(test_index)))\n",
    "    model = LinearRegression()\n",
    "    model.fit(data[train_index],target[train_index])\n",
    "\n",
    "    y_predicted = model.predict(data[test_index])\n",
    "    rmse = model.rmse(target[test_index], y_predicted)\n",
    "    models.append(model)\n",
    "    models_rmse.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.783711084159646"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(np.array(models_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
