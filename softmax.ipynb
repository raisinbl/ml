{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "- Sigmoid function:\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- Sigmoid derivative:\n",
    "$\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hàm mục tiêu\n",
    "\n",
    "Là bài toán phân Binary Classification, mong muốn output sẽ là [0, 1]<br>\n",
    "ta có thể giả sử rằng xác suất để một điểm dữ liệu $x$ rơi vào class 1 là $f(Wx)$ và rơi vào class 0 là $1−f(Wx)$.<br>\n",
    "Đặt $z_i = f(Wx_i)$ là dự đoán của $x_i$\n",
    "- nếu $y_i = 1 \\rightarrow P(y_i = 1|W,x_i) = z_i$. \n",
    "- nếu $y_i = 0 \\rightarrow P(y_i = 0|W,x_i) = 1-z_i$. \n",
    "- Hay nói cách khác: $P(y_i|W,x_i) = z_i^{y_i}({1-z_i})^{1-y_i}$\n",
    "\n",
    "Vì vậy, ta giả sử các điểm dữ liệu sinh ra độc lập với nhau:<br>\n",
    "    $ \\Rightarrow$ Với dataset, hàm mục tiêu: $P(Y|W,X) = \\prod\\limits_{i = 1}^{n}  z_i^{y_i}({1-z_i})^{1-y_i}$ <br>\n",
    "    $ \\Rightarrow$ Khi đó ta mong muốn $P(Y|W,X)$ đạt max, khi đó bài toán sẽ trở thành $\\argmax\\limits_{W}P(Y|W,X)$  tức tìm W để hàm $P(Y|W,X)$ tối ưu (Maximum Likelihood Estimator)\n",
    "  - tối ưu $~~P(Y|W,X)$ phức tạp, ta sẽ đi tối ưu min $~-\\log(P(Y|W,X)) = -\\sum\\limits_{i = 1}^{n}  {y_i}\\log({z_i})+({1-y_i})\\log({1-z_i})$, rõ ràng đây là môt hàm [cross entropy](###Cross-Entropy)\n",
    "  - Đặt $J(W,X,Y) = -\\log(P(Y|W,X))$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cập nhật \n",
    "\n",
    "Với hàm activation là [sigmoid](###Sigmoid) ta cũng có được\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w}; \\mathbf{x}_i, y_i)}{\\partial \\mathbf{w}} = (z_i - y_i)\\mathbf{x}_i$$\n",
    "\n",
    "SGD cho từng điểm dữ liệu: \n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{w} + \\eta(y_i - z_i)\\mathbf{x}_i$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dùng phương pháp xấp xỉ để kiểm tra đạo hàm\n",
    "$$f’(x) = \\lim_{\\varepsilon \\rightarrow 0}\\frac{f(x + \\varepsilon) - f(x)}{\\varepsilon}\\newline$$\n",
    "\n",
    "$$f’(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2\\varepsilon} ~~~~ (2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, lr=0.01, epochs=5, batch_size=20):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.random.rand(n_features)\n",
    "        self.b = 0\n",
    "        w_latest = self.w.copy()\n",
    "        stop_condtion = False\n",
    "        # train\n",
    "        for epoch in range(self.epochs):\n",
    "            if stop_condtion == False: break\n",
    "\n",
    "            idxs = np.random.permutation(n_samples)\n",
    "            # mini-batch gradient descent\n",
    "            for i in range(0,n_samples,self.batch_size):\n",
    "                idx = idxs[i:min(i + self.batch_size, n_samples)]\n",
    "                X_batch,y_batch = X[idx],y[idx]\n",
    "\n",
    "                z_batch = self.sigmoid(X_batch@self.w + self.b)\n",
    "                gradient = (1/self.batch_size)*(y_batch - z_batch)\n",
    "                self.w += self.lr*X_batch.T@gradient\n",
    "                self.b -= self.lr*np.sum(gradient)\n",
    "                # stop condtion\n",
    "                if np.allclose(w_latest, self.w, atol=0.001):\n",
    "                    stop_condtion = True\n",
    "                    break\n",
    "                else:\n",
    "                    w_latest = self.w.copy()\n",
    "                \n",
    "    def predict(self,X):\n",
    "        y_pred = X@self.w + self.b\n",
    "        return [0 if y<=0.7 else 1 for y in y_pred]\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        return 1/(1+ np.exp(-input))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6052631578947368\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "clf = LogisticRegression(lr=0.001,batch_size=5,epochs=10)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)\n",
    "\n",
    "acc = accuracy(y_pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "$Softmax(X) = \\frac{e^{x_i}}{\\sum\\limits_{i=1}^{n} e^{x_i}}$\n",
    "\n",
    "### Cross Entropy \n",
    "\n",
    "$H(P,Q) = {\\sum\\limits_{i=1}^C p_i \\log(q_i)}$\n",
    "Using Cross Entropy instead of Distance function because of properties: \n",
    " - Reach minimal value when $p_i$ = $q_i$\n",
    " - but when $q_i$ too far away from $p_i$, the Cross entropy get really large -> benefit when optimize loss\n",
    "\n",
    "<u>Note</u>: $H(P,Q) ≠ H(Q,P)$, điều này dễ nhận ra bởi vì nhìn vào công thức, $P$ có thể nhận giá trị 0, trong khi $Q$ bên trong hàm $\\log$ không thể  nhận giá trị 0 <br>=> In Practical, $P$ dùng làm *giá trị thực* (0,0,1,...,0), $Q$ dùng *giá trị dự đoán*.<br>\n",
    "Cross Entropy dùng để đo khoảng cách giữa hai phân phối\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loss for Softmax Regression\n",
    "\n",
    "- Loss for one point $(x_i, y_i)$: \n",
    "  - $L(W,x_i,y_i) = {y_i}\\log({a_i}) = \\sum\\limits_{j=1}^{C}{y_{ji}}\\log({a_{ji}})$<br>\n",
    "  Trong đó, $x_i, y_i$ là một cặp điểm dữ liệu; $~~y_j$ đã được one-hot encode; $~~~a_i = Softmax(y_i)$;   $~~~y_{ji}, a_{ji}$ tương ứng là class thứ j trong vector $y_j, a_j$\n",
    "- Gradient for one point $(x_i, y_i)$:\n",
    "  - We defined: $J_i(\\mathbf{W}) \\triangleq J(\\mathbf{W}; \\mathbf{x}_i, \\mathbf{y}_i)$\n",
    "  - $\\frac{\\partial J_i(W)}{\\partial w_j} = e_{ji}x_i ~(\\text{where}~ e_{ji} = a_{ji} - y_{ji}) ~~$\n",
    "  - => $\\frac{\\partial J_i(W)}{\\partial W} = x_i[e_{1i},e_{2i},...,e_{Ci}] = x_ie_i^T$\n",
    "  \n",
    "- Loss and Gradient for Training dataset:<br>\n",
    "  - $L(W,X,Y) = Y\\log(A) = \\sum\\limits_{i=1}^{N}{y_i}\\log({a_i}) = \\sum\\limits_{i=1}^{N}\\sum\\limits_{j=1}^{C}{y_{ji}}\\log({a_{ji}})$\n",
    "  - $\\frac{\\partial J(W)}{\\partial W} = XE^T$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Weight\n",
    "\n",
    "Batch Grident Decents: \n",
    "$\\mathbf{W} = \\mathbf{W} +\\eta \\mathbf{x}_{i}(\\mathbf{y}_i - \\mathbf{a}_i)^T$\n",
    "\n",
    "### Recall\n",
    "\n",
    "Công thức cập nhật này giống hệt với Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class SoftmaxRegression():\n",
    "    def __init__(self, lr = 0.0001, epochs=100):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #transform first\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "        # some parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        _, n_class = y.shape\n",
    "        self.w = np.zeros([n_features,n_class])\n",
    "        self.b = 0\n",
    "        w_latest = self.w.copy()\n",
    "        # train\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.softmax_stable(X@self.w)\n",
    "            gradient = y-y_pred\n",
    "            self.w += 1/n_samples*self.lr*X.T@gradient\n",
    "            self.b -= 1/n_samples*self.lr*np.sum(gradient)\n",
    "            if np.allclose(w_latest, self.w):\n",
    "                break\n",
    "            else: w_latest = self.w.copy()\n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        return np.apply_along_axis(lambda row: np.exp(row)/(np.sum(np.exp(row))), 1, input)\n",
    "\n",
    "    # To avoid run into numerical overflow issues bc computing the exponential of large numbers,.\n",
    "    @staticmethod\n",
    "    def softmax_stable(input):\n",
    "        input_shifted = input - np.max(input, axis=1, keepdims=True)\n",
    "        return np.apply_along_axis(lambda row: np.exp(row) / np.sum(np.exp(row)), 1, input_shifted)\n",
    "\n",
    "    def predict(self,X):\n",
    "        y_pred = self.softmax(X@self.w)\n",
    "        return np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits, load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = load_digits()\n",
    "X,y = data.data,data.target\n",
    "# with some dataset like iris,breast, we need to scale the data to achive better result\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=24)\n",
    "\n",
    "# Our model \n",
    "clf = SoftmaxRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_man = clf.predict(X_test)\n",
    "\n",
    "# sklearn model\n",
    "clf = LogisticRegression(multi_class='multinomial',solver='lbfgs')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)\n",
    "\n",
    "print(accuracy(y_pred_man,y_test),accuracy(y_pred,y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take away"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression??\n",
    "\n",
    "as we can see, both Softmax and Logistic Regression are really useful on Classification Problems. \n",
    "\n",
    "Logistic Regression apply Sigmoid function on output, while Softmax Regression apply Softmax function. The objective of they are trying to *find hyperplane* that can seperate the data\n",
    "\n",
    "They are both having very nice gradient, which can be generalized as $$gradient = y-output$$\n",
    "which `output` is the prediction: $output = activation\\_function(W.X)$; and `y` is the origin class. <br>\n",
    "After that, we can update weights and bias as: \n",
    "```\n",
    "    weight += learning_rate*X@gradient\n",
    "    bias += learning_rate*gradient\n",
    "```\n",
    "\n",
    "Take a look back how Linear Regression using Gradient Descent to optimization. We can reallize the way Softmax and Logistic Regression calculate gradient and update parameters is exactly the same as Linear Regression thanks to apply flexible Loss and activation funciton. I think that how they called *REGRESSION* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why dont we just using Loss function MSE like Linear Regression instead of Cross Entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure is ilustrated the comparation with MSE with Cross Entropy, they both get the minimal value at $q=p$, but more important, Corss Entropy 'fine' the mis-classification data point very much\n",
    "\n",
    "<img src=\"utils/img/sigmoid-softmax/cross-entropy-vs-mse.png\" alt=\"cross entropy vs mse\" width=\"800\" height=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheatsheet\n",
    "\n",
    "|                   | Logistic Regression | Softmax Regression |\n",
    "| ------------      | ------------------- | ------------------ |\n",
    "|**Using for**| Binary Classification | Multi-Class Classification |\n",
    "|**Activation Function** | Sigmoid | Softmax |\n",
    "|**Loss Function**| Negative Log Likelihood (Suprising, it has the form same as Cross Entropy) | Cross Entropy | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
