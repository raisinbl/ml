{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular Value Decomposition:\n",
    "\n",
    "N√≥ ch·ªâ ƒë∆°n thu·∫ßn n√≥i v·ªÅ vi·ªác: m·ªôt ma tr·∫≠n $A$ b·∫•t k·ª≥ n√†o ƒë·ªÅu c√≥ th·ªÉ ph√¢n t√°ch th√†nh 3 ma tr·∫≠n $U, D, V$.\n",
    "> $A = U.D.V^T$\n",
    "> \n",
    "> Trong ƒë√≥:\n",
    "> \n",
    "> $U, V$ l√† hai <u>ma tr·∫≠n tr·ª±c giao (Orthogonal Matrix)</u>, ch·ª©a c√°c Vecto ri√™ng(Eigenvector) c·ªßa $A.A^T , A^T.A$<br>\n",
    "> Ma tr·∫≠n $U, V$ tr·ª±c giao b·ªüi v√¨ t√≠ch hai ma tr·∫≠n $A$ v√† ma tr·∫≠n chuy·ªÉn v·ªã $A^T$ l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng (li√™n h·ªá sang statistic => covariance matrix) m√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng b·∫•t k·ª≥ lu√¥n c√≥ eigenmatrix l√† m·ªôt ma tr·∫≠n tr·ª±c giao\n",
    "> \n",
    "> >In general, for any matrix, the eigenvectors are NOT always orthogonal. But for a special type of matrix, symmetric matrix, the eigenvalues are always real and the corresponding eigenvectors are always orthogonal.<br>\n",
    "> >For any matrix M with n rows and m columns, M multiplies with its transpose, either M*M' or M'M, results in a symmetric matrix, so for this symmetric matrix, the eigenvectors are always orthogonal.<br>\n",
    "> > --[Yue Tyler Jin](https://math.stackexchange.com/a/2154178/958533)\n",
    "> \n",
    ">> ma tr·∫≠n tr·ª±c giao th∆∞·ªùng ƒë∆∞·ª£c ·ª©ng d·ª•ng trong ph√©p quay v·ªõi m·ªôt s·ªë t√≠nh ch·∫•t ƒë·∫∑c bi·ªát nh∆∞:\n",
    ">>> - ƒê·ªãnh th·ª©c(Determination) = $\\pm1$ (ƒë·ªãnh th·ª©c ƒë∆∞·ª£c d√πng ƒë·ªÉ di·ªÖn t·∫£ m·ª©c ƒë·ªô n√©n \"squish - stretch\" c·ªßa m·ªôt ƒë∆°n v·ªã di·ªán t√≠ch khi th·ª±c hi·ªán m·ªôt ph√©p <u>bi·∫øn ƒë·ªïi(transformation)</u> ma tr·∫≠n t·ª´ c∆° s·ªü n√†y sang c∆° s·ªü kh√°c), ƒëi·ªÅu ƒë√≥ ƒë·ªìng nghƒ©a l√† khi th·ª±c hi·ªán ph√©p bi·∫øn ƒë·ªïi v·ªõi Orthogonal Matrix, s·∫Ω ƒë∆∞·ª£c b·∫£o to√†n v·ªÅ ƒë·ªô d√†i c·ªßa vector c≈©ng nh∆∞ g√≥c gi·ªØa c√°c vector. <br> Tham kh·∫£o: [Determinant - 3Blue1Brown](https://youtu.be/Ip3X9LOh2dk),\n",
    ">>> - $A^T = A^{-1}$ -> vi·ªác bi·∫øn ƒë·ªïi ma tr·∫≠n chuy·ªÉn v·ªã s·∫Ω ƒë·ª° t·ªën k√©m r·∫•t nhi·ªÅu so v·ªõi vi·ªác ph·∫£i t√≠nh to√°n to√†n ma tr·∫≠n ngh·ªãch ƒë·∫£o => c√≥ th·ªÉ d√πng $A^T$ thay cho $A^{-1}$ trong nhi·ªÅu t√¨nh hu·ªëng\n",
    ">>\n",
    ">> Tr·ªã ri√™ng, Vecto ri√™ng (Eigenvalue, Eigenvector) c·ªßa m·ªôt ma tr·∫≠n ƒë∆∞·ª£c di·ªÖn t·∫£ qua c√¥ng th·ª©c: $A.\\vec{v} = \\lambda.\\vec{v}$<br>\n",
    ">>> - T·ª©c l√†, t·∫°i m·ªôt Kh√¥ng gian vecto $B$, khi th·ª±c hi·ªán ph√©p bi·∫øn ƒë·ªïi sang kh√¥ng gian vecto $A$ th√¨ m·ªçi vecto c·ªßa kh√¥ng gian vecto ƒë√≥ s·∫Ω b·ªã thay ƒë·ªïi h∆∞·ªõng(direction) tr·ª´ c√°c vecto ƒë·∫∑c bi√™t $\\vec{v}$, c√°c vecto ƒë·∫∑c bi·ªát $\\vec{v}$ n√†y g·ªçi l√† *eigenvector* v√† t∆∞∆°ng ·ª©ng v·ªõi m·ªói *eigenvector* s·∫Ω l√† m·ªôt *eigenvalue*,  *eigenvalue* l√† m·ªôt ƒë·∫°i l∆∞·ª£ng th·ªÉ hi·ªán ƒë·ªô stretch c·ªßa m·ªôt *eigenvector* ·ªü kh√¥ng gian m·ªõi <br>Tham kh·∫£o: [Eigenvectors and eigenvalues - 3Blue1Brown](https://youtu.be/PFDu9oVAE-g) <br>\n",
    ">>> - ·ª©ng d·ª•ng trong vi·ªác t√≠nh to√°n ma tr·∫≠n, v√≠ d·ª• $A^{100}$ c≈©ng ch·ªâ t∆∞∆°ng ƒë∆∞∆°ng v·ªõi $V^{-1}\\lambda^{100}V$ => gi·∫£m kh·ªëi l∆∞·ª£ng t√≠nh to√°n r·∫•t nhi·ªÅu\n",
    "\n",
    "> n√£y gi·ªù n√≥i term  bi·∫øn ƒë·ªïi(transformation), bi·∫øn ƒë·ªïi kh√¥ng gian v√©cto n√†y sang kh√¥ng gian vect∆° kh√°c, th√¨ bi·∫øn ƒë·ªïi ·ªü ƒë√¢y ƒë∆°n gi·∫£n l√† nh√¢n hai ma tr·∫≠n v·ªõi nhau, $A . B$, theo th·ª© t·ª± n√†y th√¨ t·ª©c l√† bi·∫øn ƒë·ªïi kh√¥ng gian v√©c t∆° c·ªßa B v·ªÅ kh√¥ng gian v√©c t∆° c·ªßa A, n√≥i d·ªÖ hi·ªÉu h∆°n n·ªØa t·ª©c l√† th·ªÉ hi·ªán m·ªôt v√©c t∆° d∆∞·ªõi g√≥c nh√¨n c·ªßa kh√¥ng gian B sang g√≥c nh√¨n c·ªßa kh√¥ng gian A \n",
    "\n",
    "> Khi t√¨m ƒë∆∞·ª£c U v√† V r·ªìi th√¨ ·ª©ng d·ª•ng l·∫°i t√≠nh ch·∫•t c·ªßa Orthogonal matrix l·∫°i ƒë·ªÉ t√¨m D<br>\n",
    "> r·∫•t hay ·ªü m·ªôt ch·ªó  <u>D l√† m·ªôt ma tr·∫≠n ƒë∆∞·ªùng ch√©o(Diagonal matrix)</u> trong ƒë√≥ c√°c ph·∫ßn t·ª≠ tr√™n ƒë∆∞·ªùng ch√©o c·ªßa D s·∫Ω c√≥ gi√° tr·ªã kh√¥ng √¢m, gi·∫£m d·∫ßn<br>\n",
    "> => v√¨ gi·∫£m d·∫ßn n√™n ta c√≥ th·ªÉ ch·ªçn K c·ªôt ƒë·∫ßu ti√™n c·ªßa D ƒë·ªÉ x·∫•p x·ªâ ma tr·∫≠n A ban ƒë·∫ßu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> T√≥m g·ªçn trong b√†i to√°n SVD, eigenvector d√πng ƒë·ªÉ t√¨m ra hai ma tr·∫≠n $U, V$, hai ma tr·∫≠n n√†y l√† hai ma tr·∫≠n tr·ª±c giao => r·∫•t d·ªÖ ƒë·ªÉ t√≠nh to√°n<br>\n",
    "qua ƒë√≥, quay ng∆∞·ª£c l·∫°i ƒë·ªÉ t√≠nh ma tr·∫≠n ch√©o $D$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement  <br>\n",
    "\n",
    "- First, find V and U, by just find the Eigenmatrix of $A^T.A$ and $A.A^T$ <br>\n",
    "- After having V and U, subtitute back to find Sigma(Diagonal Matrix) by: $D = U^T.A.V$\n",
    "\n",
    "From this [video](https://www.youtube.com/watch?v=4tvw-1HI45s)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVD](utils/img/svd-pca/svd.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import lib and init matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd, eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "np.random.seed(42)\n",
    "A = np.random.rand(10, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `eig` return eigenvalues and eigenmatrix with columns as normalized eigenvectors\n",
    "ev_U, U = eig(A@A.T)\n",
    "ev_V, V = eig(A.T@A)\n",
    "# After finding U and V, subtitute back them to find Diagonal matrix\n",
    "Sigma = U.T@A@V\n",
    "\n",
    "# Reconstruct A\n",
    "A_recon = U@Sigma@V.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy.linalg.SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SVD built in function of numpy to test the result\n",
    "U_np, D, V_np_T = svd(A)\n",
    "\n",
    "# Reconstruct A \n",
    "\n",
    "# diagonalize D, and padding zeros to make it same size as right singular matrix\n",
    "sigma = np.pad(np.diag(D), ((0, A.shape[0]-A.shape[1] if A.shape[0]>A.shape[1] else 0), (0, A.shape[1]-A.shape[0] if A.shape[0]<A.shape[1] else 0)), 'constant', constant_values=0)\n",
    "\n",
    "A_np_recon = U_np@sigma@V_np_T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There a slightly different in the result of U,D,V when calculate manually and by numpy function, but it is okay, because `numpy` has been take the absolute value and sorted on Diagonal matrix, but that's okay because:<br>\n",
    "\n",
    "![image.png](utils/img/svd-pca/svd_truncated.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ n√™n ƒë·ªÉ c√≥ SVD gi·ªëng v·ªõi numpy th√¨ c·∫ßn sort eigenvalue ·ªü D theo abs, c√≥ th·ªÉ l√†m c√°i mask c≈©ng ƒë∆∞·ª£c r·ªìi ƒë∆∞a c√°c c·ªôt ·ªü U, V_T theo v·ªã tr√≠ c·ªßa D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "## Note"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain why eigenvalue cruical in this problem\n",
    "[Answer by @amoeba](https://stats.stackexchange.com/a/219344)\n",
    "\n",
    "#### Problem statement\n",
    "\n",
    "> The geometric problem that PCA is trying to optimize is clear to me: PCA tries to find the first principal component by minimizing the reconstruction (projection) error, which simultaneously maximizes the variance of the projected data.\n",
    "\n",
    "....\n",
    "\n",
    "Let's take the second formulation: **PCA is trying the find the direction such that the projection of the data on it has the highest possible variance**. This direction is, by definition, called the first principal direction. We can formalize it as follows: given the covariance matrix $C$, we are looking for a vector $w$ having unit length, $‚à•w‚à•=1$, such that $w^‚ä§Cw$ is maximal.\n",
    "\n",
    "(Just in case this is not clear: if $X$ is the centered data matrix, then the projection is given by $Xw$ and its variance is\n",
    " $ \\frac{1}{n-1}(Xw)^{\\top} \\cdot Xw = w^{\\top} \\cdot \\left(\\frac{1}{n-1}X^{\\top}X\\right) \\cdot w = w^{\\top}Cw $)\n",
    "\n",
    "On the other hand, an eigenvector of $C$ is, by definition, any vector $v$ such that $Cv=Œªv$.\n",
    "\n",
    "**It turns out that the first principal direction is given by the eigenvector with the largest eigenvalue. This is a nontrivial and surprising statement.**\n",
    "\n",
    "-----------------------------------------------------------\n",
    "\n",
    "#### Proofs\n",
    "\n",
    "If one opens any book or tutorial on PCA, one can find there the following almost one-line proof of the statement above. We want to maximize $w^‚ä§Cw$ under the constraint that $‚à•w‚à•=w‚ä§w=1$; <br>this can be done introducing a <u>Lagrange multiplier and maximizing $w^‚ä§Cw‚àíŒª(w^‚ä§w‚àí1)$</u>; differentiating, we obtain $Cw‚àíŒªw=0$, which is the eigenvector equation. We see that $Œª$ has in fact to be the largest eigenvalue by substituting this solution into the objective function, which gives $w^‚ä§Cw‚àíŒª(w^‚ä§w‚àí1)=w^‚ä§Cw=Œªw^‚ä§w=Œª$\n",
    ". By virtue of the fact that this objective function should be maximized, $Œª$\n",
    " must be the largest eigenvalue, QED.\n",
    "\n",
    "This tends to be not very intuitive for most people.\n",
    "\n",
    "-----------------------------------------------------------\n",
    "\n",
    "### T√≥m l·∫°i:\n",
    "\n",
    "V·∫•n ƒë·ªÅ c·∫ßn l√†m ƒë√≥ l√†: Maximize c√°i Variance c·ªßa Data khi chi·∫øu l√™n chi·ªÅu kh√¥ng gian m·ªõi v√† c√°i gi√° tr·ªã maximize ƒë·∫°t ƒë∆∞·ª£c b·∫±ng the-maximum-eigenvalue $Œª$ (ch·ª©ng minh tr√™n); d·∫•u b·∫±ng x·∫£y ra khi v√† ch·ªâ khi $w$ l√† eigenvector c·ªßa eigenvalue $Œª$ ƒë√≥, v√† ƒëi·ªÅu ƒë√≥ c≈©ng n√≥i ƒë∆∞·ª£c r·∫±ng l√† $w$ ch√≠nh l√† nh·ªØng basis eigenvector c·ªßa Covariance Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to reverse PCA and reconstruct original variables from several principal components?\n",
    "\n",
    "[Answer by @amoeba](https://stats.stackexchange.com/a/229093)\n",
    "\n",
    "r√µ r√†ng gi·ªù ta ƒë√£ bi·∫øt, gi·ªù v·ªõi t·∫≠p h·ª£p eigenvectors c·ªßa Covariance Matrix, th√¨ ta c√≥ th·ªÉ chi·∫øu Data X xu·ªëng chi·ªÅu kh√¥ng gian vector c·ªßa eigenvectors (t·∫°m g·ªçi l√† $V$) m√† v·∫´n gi·ªØ ƒë∆∞·ª£c to√†n b·ªô l∆∞·ª£ng Variance, gi·∫£ s·ª≠ V ƒë√£ gi·ªØ nh·ªØng eigenvectors n·∫Øm ƒëa s·ªë l∆∞·ª£ng th√¥ng tin c·∫ßn thi·∫øt\n",
    "\n",
    "Data m·ªõi t·∫°i chi·ªÅu kh√¥ng gian vector V: $Z = X.V$\n",
    "\n",
    "$=>$ recontruct X ki·ªÉu g√¨?: ƒë∆°n gi·∫£n $Z.V^{-1}$ m√† V l√† ma tr·∫≠n tr·ª±c giao th·∫ø n√™n l√† $Z.V^{-1} = Z.V^T$ -> ƒëi·ªÅu ƒë·∫•y s·∫Ω c√†ng n√≥i th√™m khi m√† c√°i V gi·ªØ c√†ng nhi·ªÅu eigenvector, th√¨ l∆∞·ª£ng th√¥ng tin ƒë∆∞·ª£c b·∫£o to√†n s·∫Ω c√†ng l·ªõn, n·∫øu d√πng nguy√™n ma tr·∫≠n V ban ƒë·∫ßu s·∫Ω gi·ªØ ƒë∆∞·ª£c nguy√™n X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA objective function: what is the connection between maximizing variance and minimizing error?\n",
    "\n",
    "[Answer by @amoeba](https://stats.stackexchange.com/a/136072)\n",
    "\n",
    "<img src=\"utils/img/svd-pca/pca_is_not_linear_regression.png\" alt=\"image\" width=\"800\" height=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between SVD and PCA. How to use SVD to perform PCA?\n",
    "\n",
    "[Answer by @amoeba](https://stats.stackexchange.com/a/134283)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "\n",
    "## Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"utils/img/svd-pca/pca_procedure.png\" alt=\"image\" width=\"800\" height=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import lib and init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig, svd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed and generate random matrix\n",
    "\n",
    "np.random.seed(42)\n",
    "A = np.random.rand(10, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "A_std = (A - A.mean(axis=0))\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov = np.cov(A_std, rowvar=False)\n",
    "\n",
    "# Find eigenvalues and eigenvectors of covariance matrix\n",
    "ev, V = eig(cov)\n",
    "\n",
    "# Sort eigenvalues and eigenvectors\n",
    "idx = ev.argsort()[::-1]\n",
    "ev = ev[idx]\n",
    "V = V[:, idx]\n",
    "\n",
    "# Find principal components | Projection of A_std to V\n",
    "PC = A_std@V\n",
    "\n",
    "# Reconstruct A | The data point in new space\n",
    "A_recon = PC@V.T\n",
    "\n",
    "# Calculate percentage of variance explained by each principal components\n",
    "var_exp = np.cumsum(ev)/np.sum(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of principal components that explain 95% of variance\n",
    "n = np.argmax(var_exp>0.95)+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn PCA to test the result\n",
    "pca = PCA(n_components=n)\n",
    "\n",
    "pca = pca.fit(A_std)\n",
    "\n",
    "# new data points in new space\n",
    "A_recon_lib = pca.transform(A_std)\n",
    "\n",
    "# percentage of variance explained by each principal components\n",
    "var_exp_lib = pca.explained_variance_ratio_\n",
    "\n",
    "# principal components\n",
    "PC_lib = pca.components_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
